{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18167\n",
      "18167\n",
      "[['i', 'love', 'you', '.'], ['i', 'miss', 'you', '.'], ['i', 'need', 'you', '.'], ['i', 'think', 'so', '.'], ['i', 'use', 'this', '.']]\n",
      "[['SOS', '我', '爱', '您', '。', 'EOS'], ['SOS', '我', '想', '念', '你', '。', 'EOS'], ['SOS', '我', '需', '要', '你', '。', 'EOS'], ['SOS', '我', '想', '是', '這', '樣', '的', '。', 'EOS'], ['SOS', '我', '使', '用', '这', '个', '。', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "file_path = './data/cmn.txt'\n",
    "START_TOKEN = 'SOS'\n",
    "END_TOKEN = 'EOS'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    total_lines = sum(1 for line in file)\n",
    "\n",
    "lines_to_read = int(1 * total_lines)\n",
    "\n",
    "random_lines = random.sample(range(total_lines), lines_to_read)\n",
    "\n",
    "en = []\n",
    "cn = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line_number, line in enumerate(file):\n",
    "        if line_number in random_lines:\n",
    "            line = line.strip().split('\\t')\n",
    "            en.append(word_tokenize(line[0].lower()))\n",
    "            cn.append([START_TOKEN] + word_tokenize(\" \".join([w for w in line[1]])) + [END_TOKEN])\n",
    "\n",
    "print(len(en))\n",
    "print(len(cn))\n",
    "\n",
    "print(en[201:206])\n",
    "print(cn[201:206])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_len = [len(s) for s in en]\n",
    "cn_len = [len(s) for s in cn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "axs[0].hist(en_len, bins = 10)\n",
    "axs[0].set(ylim=(0, 100))\n",
    "axs[1].hist(cn_len, bins = 10)\n",
    "axs[1].set(ylim=(0, 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter sentences by max_sequence_length of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 32\n",
    "valid_en = []\n",
    "valid_cn = []\n",
    "for i in range(len(en)):\n",
    "    if len(en[i]) <= max_sequence_length and len(cn[i]) <= max_sequence_length:\n",
    "        valid_en.append(en[i])\n",
    "        valid_cn.append(cn[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of en tokens 6022\n",
      "number of cn tokens 3354\n"
     ]
    }
   ],
   "source": [
    "en_all_tokens = []\n",
    "for en_sentence in valid_en:\n",
    "    for token in en_sentence:\n",
    "        en_all_tokens.append(token)\n",
    "\n",
    "en_token_counts = Counter(en_all_tokens)\n",
    "en_common_tokens = en_token_counts.most_common(10000)\n",
    "print(f\"number of en tokens {len(en_common_tokens)}\")\n",
    "en_token_dict = {token: index for index, (token, _) in enumerate(en_common_tokens)}\n",
    "en_token_dict['UNK'] = len(en_common_tokens)\n",
    "en_token_dict['PAD'] = len(en_common_tokens) + 1\n",
    "\n",
    "cn_all_tokens = []\n",
    "for cn_sentence in valid_cn:\n",
    "    for token in cn_sentence:\n",
    "        cn_all_tokens.append(token)\n",
    "\n",
    "cn_token_counts = Counter(cn_all_tokens)\n",
    "cn_common_tokens = cn_token_counts.most_common(10000)\n",
    "print(f\"number of cn tokens {len(cn_common_tokens)}\")\n",
    "cn_token_dict = {token: index for index, (token, _) in enumerate(cn_common_tokens)}\n",
    "cn_token_dict['UNK'] = len(cn_common_tokens)\n",
    "cn_token_dict['PAD'] = len(cn_common_tokens) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18153\n",
      "18153\n",
      "[tensor([   0, 1771,    2,    1, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355]), tensor([   0,    6,   33,    2,    1, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355]), tensor([   0,    6,   86,  397,    4,    2,    1, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355])]\n"
     ]
    }
   ],
   "source": [
    "en_tokenized = []\n",
    "for en_sentence in valid_en:\n",
    "    if len(en_sentence) < max_sequence_length:\n",
    "        en_sentence = en_sentence + ['PAD'] * (max_sequence_length - len(en_sentence))\n",
    "    en_tokenized.append(torch.tensor([en_token_dict.get(token, en_token_dict['UNK']) for token in en_sentence]))\n",
    "\n",
    "cn_tokenized = []\n",
    "for cn_sentence in valid_cn:\n",
    "    if len(cn_sentence) < max_sequence_length:\n",
    "        cn_sentence = cn_sentence + ['PAD'] * (max_sequence_length - len(cn_sentence))\n",
    "    cn_tokenized.append(torch.tensor([cn_token_dict.get(token, cn_token_dict['UNK']) for token in cn_sentence]))\n",
    "\n",
    "print(len(en_tokenized))\n",
    "print(len(cn_tokenized))\n",
    "print(cn_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, en_tokenized_sentences, cn_tokenized_sentences):\n",
    "        self.en_tokenized = en_tokenized_sentences\n",
    "        self.cn_tokenized = cn_tokenized_sentences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.en_tokenized)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.en_tokenized[idx], self.cn_tokenized[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenizedDataset(en_tokenized, cn_tokenized)\n",
    "\n",
    "# Define the size of your train and test datasets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # 20% for testing\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for train and test datasets\n",
    "batch_size = 3\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32])\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(train_loader))\n",
    "print(first_batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(eng_batch, cn_batch, eng_pad_token, cn_pad_token):\n",
    "    # Create masks based on padding tokens\n",
    "    eng_mask = (eng_batch == eng_pad_token)\n",
    "    cn_mask = (cn_batch == cn_pad_token)\n",
    "\n",
    "    # Create Look-Ahead Mask\n",
    "    max_sequence_length = eng_batch.size(1)  # Assuming both batches have the same sequence length\n",
    "    look_ahead_mask = torch.triu(torch.ones(max_sequence_length, max_sequence_length), diagonal=1) == 1\n",
    "\n",
    "    # Expand masks to 3D for self-attention and cross-attention\n",
    "    encoder_padding_mask = eng_mask.unsqueeze(1).repeat(1, max_sequence_length, 1)\n",
    "    decoder_padding_mask_self_attention = cn_mask.unsqueeze(1).repeat(1, max_sequence_length, 1)\n",
    "    decoder_padding_mask_cross_attention = eng_mask.unsqueeze(1).repeat(1, max_sequence_length, 1)\n",
    "\n",
    "    # Calculate the final masks with some negative infinity value for masked positions\n",
    "    NEG_INFTY = -1e9\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask = torch.where(look_ahead_mask.unsqueeze(0) + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3355\n"
     ]
    }
   ],
   "source": [
    "eng_pad_token = en_token_dict.get('PAD')\n",
    "cn_pad_token = cn_token_dict.get('PAD')\n",
    "print(cn_pad_token)\n",
    "# Assuming eng_batch and cn_batch are your input tensors\n",
    "encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(first_batch[0], first_batch[1], eng_pad_token, cn_pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(decoder_cross_attention_mask.shape)\n",
    "print(len(cn_token_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer, get_device\n",
    "\n",
    "d_model = 256\n",
    "batch_size = 3\n",
    "ffn_hidden = 512\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "num_layers = 1\n",
    "\n",
    "transformer = Transformer(en_token_dict, cn_token_dict, d_model, max_sequence_length, ffn_hidden, num_heads, drop_prob, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(6024, 256)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialEncoder(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(3356, 256)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialDecoder(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (cross_attention): MultiHeadCrossAttention(\n",
       "          (q_layer): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (kv_layer): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=3356, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=cn_token_dict.get('PAD'), reduction='none')\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if item is Chinese character\n",
    "# cn_end_marks_filtered = []\n",
    "# for item in cn_end_marks_set:\n",
    "#     if not re.search(\"[\\u4e00-\\u9FFF]\", item):\n",
    "#         cn_end_marks_filtered.append(item)\n",
    "# print(cn_end_marks_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "NEG_INFTY = -1e9\n",
    "eng_batch = torch.tensor([[23, 7, 5, 1, 1], [15, 25, 2, 6, 1]])\n",
    "cn_batch = torch. tensor([[6, 8, 1, 1, 1], [19, 5, 3, 1, 1]])\n",
    "padding_token = 1\n",
    "eng_mask = (eng_batch == padding_token)\n",
    "cn_mask = (cn_batch == padding_token)\n",
    "decoder_padding_mask_self_attention = cn_mask.unsqueeze(1).repeat(1, 5, 1)\n",
    "print(decoder_padding_mask_self_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look ahead mask:\n",
      " tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]])\n",
      "\n",
      "decoder_padding_mask_cross_attention:\n",
      " tensor([[[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "look_ahead_mask = torch.triu(torch.ones(5, 5), diagonal=1) == 1\n",
    "print(f\"look ahead mask:\\n {look_ahead_mask}\\n\")\n",
    "decoder_padding_mask_cross_attention = look_ahead_mask.unsqueeze(0) + decoder_padding_mask_self_attention\n",
    "print(f\"decoder_padding_mask_cross_attention:\\n {decoder_padding_mask_cross_attention}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[ 0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09]]])\n"
     ]
    }
   ],
   "source": [
    "decoder_cross_attention_mask = torch.where((decoder_padding_mask_cross_attention), NEG_INFTY, 0)\n",
    "print(decoder_cross_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2], [2, 3], [3, 4]])\n",
    "print(x.size())\n",
    "expanded_x = x.unsqueeze(0)\n",
    "print(expanded_x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
