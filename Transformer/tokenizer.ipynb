{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18167\n",
      "18167\n",
      "[['i', 'love', 'you', '.'], ['i', 'miss', 'you', '.'], ['i', 'need', 'you', '.'], ['i', 'think', 'so', '.'], ['i', 'use', 'this', '.']]\n",
      "[['SOS', '我', '爱', '您', '。', 'EOS'], ['SOS', '我', '想', '念', '你', '。', 'EOS'], ['SOS', '我', '需', '要', '你', '。', 'EOS'], ['SOS', '我', '想', '是', '這', '樣', '的', '。', 'EOS'], ['SOS', '我', '使', '用', '这', '个', '。', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "file_path = './data/cmn.txt'\n",
    "START_TOKEN = 'SOS'\n",
    "END_TOKEN = 'EOS'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    total_lines = sum(1 for line in file)\n",
    "\n",
    "lines_to_read = int(1 * total_lines)\n",
    "\n",
    "random_lines = random.sample(range(total_lines), lines_to_read)\n",
    "\n",
    "en = []\n",
    "cn = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line_number, line in enumerate(file):\n",
    "        if line_number in random_lines:\n",
    "            line = line.strip().split('\\t')\n",
    "            en.append(word_tokenize(line[0].lower()))\n",
    "            cn.append([START_TOKEN] + word_tokenize(\" \".join([w for w in line[1]])) + [END_TOKEN])\n",
    "\n",
    "print(len(en))\n",
    "print(len(cn))\n",
    "\n",
    "print(en[201:206])\n",
    "print(cn[201:206])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "en_len = [len(s) for s in en]\n",
    "cn_len = [len(s) for s in cn]\n",
    "f, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "axs[0].hist(en_len, bins = 10)\n",
    "axs[0].set(ylim=(0, 100))\n",
    "axs[1].hist(cn_len, bins = 10)\n",
    "axs[1].set(ylim=(0, 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter sentences by max_sequence_length of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 32\n",
    "valid_en = []\n",
    "valid_cn = []\n",
    "for i in range(len(en)):\n",
    "    if len(en[i]) <= max_sequence_length and len(cn[i]) <= max_sequence_length:\n",
    "        valid_en.append(en[i])\n",
    "        valid_cn.append(cn[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of en tokens 6022\n",
      "number of cn tokens 3354\n"
     ]
    }
   ],
   "source": [
    "en_all_tokens = []\n",
    "for en_sentence in valid_en:\n",
    "    for token in en_sentence:\n",
    "        en_all_tokens.append(token)\n",
    "\n",
    "en_token_counts = Counter(en_all_tokens)\n",
    "en_common_tokens = en_token_counts.most_common(10000)\n",
    "print(f\"number of en tokens {len(en_common_tokens)}\")\n",
    "en_token_dict = {token: index for index, (token, _) in enumerate(en_common_tokens)}\n",
    "en_token_dict['UNK'] = len(en_common_tokens)\n",
    "en_token_dict['PAD'] = len(en_common_tokens) + 1\n",
    "\n",
    "cn_all_tokens = []\n",
    "for cn_sentence in valid_cn:\n",
    "    for token in cn_sentence:\n",
    "        cn_all_tokens.append(token)\n",
    "\n",
    "cn_token_counts = Counter(cn_all_tokens)\n",
    "cn_common_tokens = cn_token_counts.most_common(10000)\n",
    "print(f\"number of cn tokens {len(cn_common_tokens)}\")\n",
    "cn_token_dict = {token: index for index, (token, _) in enumerate(cn_common_tokens)}\n",
    "cn_token_dict['UNK'] = len(cn_common_tokens)\n",
    "cn_token_dict['PAD'] = len(cn_common_tokens) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanye\\AppData\\Local\\Temp\\ipykernel_10184\\1063115924.py:5: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  en_tokenized.append(torch.tensor([en_token_dict.get(token, en_token_dict['UNK']) for token in en_sentence]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18153\n",
      "18153\n",
      "[tensor([   0, 1771,    2,    1, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355]), tensor([   0,    6,   33,    2,    1, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355]), tensor([   0,    6,   86,  397,    4,    2,    1, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "        3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355])]\n"
     ]
    }
   ],
   "source": [
    "en_tokenized = []\n",
    "for en_sentence in valid_en:\n",
    "    if len(en_sentence) < max_sequence_length:\n",
    "        en_sentence = en_sentence + ['PAD'] * (max_sequence_length - len(en_sentence))\n",
    "    en_tokenized.append(torch.tensor([en_token_dict.get(token, en_token_dict['UNK']) for token in en_sentence]))\n",
    "\n",
    "cn_tokenized = []\n",
    "for cn_sentence in valid_cn:\n",
    "    if len(cn_sentence) < max_sequence_length:\n",
    "        cn_sentence = cn_sentence + ['PAD'] * (max_sequence_length - len(cn_sentence))\n",
    "    cn_tokenized.append(torch.tensor([cn_token_dict.get(token, cn_token_dict['UNK']) for token in cn_sentence]))\n",
    "\n",
    "print(len(en_tokenized))\n",
    "print(len(cn_tokenized))\n",
    "print(cn_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, en_tokenized_sentences, cn_tokenized_sentences):\n",
    "        self.en_tokenized = en_tokenized_sentences\n",
    "        self.cn_tokenized = cn_tokenized_sentences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.en_tokenized)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.en_tokenized[idx], self.cn_tokenized[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenizedDataset(en_tokenized, cn_tokenized)\n",
    "\n",
    "# Define the size of your train and test datasets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # 20% for testing\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for train and test datasets\n",
    "batch_size = 3\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  23,  216,    7,  117,  336,   21, 3470,   84,  153,    3,  937,    0,\n",
      "         6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023,\n",
      "         6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023],\n",
      "        [  28,   10,    4,   77,   13,   12,    6, 6023, 6023, 6023, 6023, 6023,\n",
      "         6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023,\n",
      "         6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023],\n",
      "        [  15,  748,   21,    4,   21,  132,  391,    0, 6023, 6023, 6023, 6023,\n",
      "         6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023,\n",
      "         6023, 6023, 6023, 6023, 6023, 6023, 6023, 6023]]), tensor([[   0,   27,   30,  179,  321,  176,  103,  115,   45,  246,  538,  137,\n",
      "          443,    5,    2,    1, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "         3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355],\n",
      "        [   0,    6,  260,   40,  160,   39,  223,   12,    1, 3355, 3355, 3355,\n",
      "         3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "         3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355],\n",
      "        [   0,   14,  219,    5,    6,  322,   23,  103,   83,    2,    1, 3355,\n",
      "         3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355,\n",
      "         3355, 3355, 3355, 3355, 3355, 3355, 3355, 3355]])]\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(train_loader))\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(eng_batch, cn_batch, eng_pad_token, cn_pad_token):\n",
    "    # Create masks based on padding tokens\n",
    "    eng_mask = (eng_batch == eng_pad_token)\n",
    "    cn_mask = (cn_batch == cn_pad_token)\n",
    "\n",
    "    # Create Look-Ahead Mask\n",
    "    max_sequence_length = eng_batch.size(1)  # Assuming both batches have the same sequence length\n",
    "    look_ahead_mask = torch.triu(torch.ones(max_sequence_length, max_sequence_length), diagonal=1) == 1\n",
    "\n",
    "    # Expand masks to 3D for self-attention and cross-attention\n",
    "    encoder_padding_mask = eng_mask.unsqueeze(1).repeat(1, max_sequence_length, 1)\n",
    "    decoder_padding_mask_self_attention = cn_mask.unsqueeze(1).repeat(1, max_sequence_length, 1)\n",
    "    decoder_padding_mask_cross_attention = eng_mask.unsqueeze(1).repeat(1, max_sequence_length, 1)\n",
    "\n",
    "    # Calculate the final masks with some negative infinity value for masked positions\n",
    "    NEG_INFTY = -1e9\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask = torch.where(look_ahead_mask.unsqueeze(0) + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_pad_token = en_token_dict.get('PAD')\n",
    "cn_pad_token = cn_token_dict.get('PAD')\n",
    "# Assuming eng_batch and cn_batch are your input tensors\n",
    "encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(first_batch[0], first_batch[1], eng_pad_token, cn_pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09])\n"
     ]
    }
   ],
   "source": [
    "print(decoder_cross_attention_mask[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if item is Chinese character\n",
    "# cn_end_marks_filtered = []\n",
    "# for item in cn_end_marks_set:\n",
    "#     if not re.search(\"[\\u4e00-\\u9FFF]\", item):\n",
    "#         cn_end_marks_filtered.append(item)\n",
    "# print(cn_end_marks_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "eng_batch = torch.tensor([[23, 7, 5, 1, 1], [15, 25, 2, 6, 1]])\n",
    "cn_batch = torch. tensor([[6, 8, 1, 1, 1], [19, 5, 3, 1, 1]])\n",
    "padding_token = 1\n",
    "eng_mask = (eng_batch == padding_token)\n",
    "cn_mask = (cn_batch == padding_token)\n",
    "decoder_padding_mask_self_attention = cn_mask.unsqueeze(1).repeat(1, 5, 1)\n",
    "print(decoder_padding_mask_self_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look ahead mask:\n",
      " tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]])\n",
      "\n",
      "decoder_padding_mask_cross_attention:\n",
      " tensor([[[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "look_ahead_mask = torch.triu(torch.ones(5, 5), diagonal=1) == 1\n",
    "print(f\"look ahead mask:\\n {look_ahead_mask}\\n\")\n",
    "decoder_padding_mask_cross_attention = look_ahead_mask.unsqueeze(0) + decoder_padding_mask_self_attention\n",
    "print(f\"decoder_padding_mask_cross_attention:\\n {decoder_padding_mask_cross_attention}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[ 0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09]]])\n"
     ]
    }
   ],
   "source": [
    "decoder_cross_attention_mask = torch.where((decoder_padding_mask_cross_attention), NEG_INFTY, 0)\n",
    "print(decoder_cross_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
